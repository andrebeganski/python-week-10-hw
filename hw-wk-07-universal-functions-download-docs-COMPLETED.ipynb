{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c4233d",
   "metadata": {},
   "source": [
    "# 1. Demo downloading files from websites \n",
    "\n",
    "There are ```txt``` and ```pdf``` files on:\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/pages.html```\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Download all ```pdf``` files.\n",
    "2. Download all files at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4f97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create new cells as necessary\n",
    "# import libraries\n",
    "from bs4 import BeautifulSoup  ## scrape info from web pages\n",
    "import requests ## get web pages from server\n",
    "import time # time is required. we will use its sleep function\n",
    "from random import randrange # generate random numbers\n",
    "import wget # can put down documents, files from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54867b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## capture response\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/pages.html\"\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9f47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making soup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d12126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"pdfs downloadable\">\n",
       "<p class=\"pages\">Download this list of PDFs</p>\n",
       "<li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
       "<li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
       "</ul>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## finding section with pdf links\n",
    "pdf_holder = soup.find(\"ul\",class_=\"pdfs\")\n",
    "pdf_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b663fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## extracting a tags from sections\n",
    "## isolating link from a tags and combining it with base url\n",
    "links_a_tag_pdf = pdf_holder.find_all(\"a\")\n",
    "all_pdf_links_fl = []\n",
    "base_url = \"https://sandeepmj.github.io/scrape-example-page/\"\n",
    "for a_tag in links_a_tag_pdf:\n",
    "    all_pdf_links_fl.append(base_url+a_tag.get(\"href\"))\n",
    "all_pdf_links_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1cc405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 10\n",
      "100% [..........................................................] 12812 / 12812snoozing for 6 before next link\n",
      "Downloading link 2 of 10\n",
      "100% [..........................................................] 12897 / 12897snoozing for 4 before next link\n",
      "Downloading link 3 of 10\n",
      "100% [..........................................................] 12908 / 12908snoozing for 3 before next link\n",
      "Downloading link 4 of 10\n",
      "100% [..........................................................] 12843 / 12843snoozing for 6 before next link\n",
      "Downloading link 5 of 10\n",
      "100% [..........................................................] 12881 / 12881snoozing for 4 before next link\n",
      "Downloading link 6 of 10\n",
      "100% [..........................................................] 12906 / 12906snoozing for 6 before next link\n",
      "Downloading link 7 of 10\n",
      "100% [..........................................................] 12816 / 12816snoozing for 3 before next link\n",
      "Downloading link 8 of 10\n",
      "100% [..........................................................] 12921 / 12921snoozing for 4 before next link\n",
      "Downloading link 9 of 10\n",
      "100% [..........................................................] 12901 / 12901snoozing for 6 before next link\n",
      "Downloading link 10 of 10\n",
      "100% [..........................................................] 13049 / 13049snoozing for 4 before next link\n"
     ]
    }
   ],
   "source": [
    "## downloading pdfs\n",
    "links_total = len(all_pdf_links_fl)\n",
    "link_count = 1\n",
    "\n",
    "\n",
    "for link in all_pdf_links_fl:\n",
    "    print(f\"Downloading link {link_count} of {links_total}\")\n",
    "    link_count += 1\n",
    "    wget.download(link)\n",
    "    snoozer = randrange(3,7)\n",
    "    print(f\"snoozing for {snoozer} before next link\")\n",
    "    time.sleep(snoozer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f429842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## grabbing downloadable sections\n",
    "docs_holder = soup.find_all(\"ul\", class_=\"downloadable\")\n",
    "\n",
    "## extracting a tags from sections\n",
    "all_doc_a_tags = [item.find_all(\"a\") for item in docs_holder]\n",
    "\n",
    "## flattening lists\n",
    "import itertools\n",
    "flat_a_tags = list(itertools.chain(*all_doc_a_tags))\n",
    "\n",
    "## pulling url from a tags and combining with base url\n",
    "doc_links = [base_url+item.get(\"href\") for item in flat_a_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba6b24f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 20\n",
      "100% [................................................................] 76 / 76snoozing for 6 before next link\n",
      "Downloading link 2 of 20\n",
      "100% [................................................................] 66 / 66snoozing for 4 before next link\n",
      "Downloading link 3 of 20\n",
      "100% [................................................................] 70 / 70snoozing for 4 before next link\n",
      "Downloading link 4 of 20\n",
      "100% [................................................................] 63 / 63snoozing for 6 before next link\n",
      "Downloading link 5 of 20\n",
      "100% [................................................................] 66 / 66snoozing for 6 before next link\n",
      "Downloading link 6 of 20\n",
      "100% [................................................................] 66 / 66snoozing for 3 before next link\n",
      "Downloading link 7 of 20\n",
      "100% [................................................................] 69 / 69snoozing for 3 before next link\n",
      "Downloading link 8 of 20\n",
      "100% [................................................................] 65 / 65snoozing for 6 before next link\n",
      "Downloading link 9 of 20\n",
      "100% [................................................................] 66 / 66snoozing for 4 before next link\n",
      "Downloading link 10 of 20\n",
      "100% [................................................................] 60 / 60snoozing for 6 before next link\n",
      "Downloading link 11 of 20\n",
      "100% [..........................................................] 12812 / 12812snoozing for 3 before next link\n",
      "Downloading link 12 of 20\n",
      "100% [..........................................................] 12897 / 12897snoozing for 3 before next link\n",
      "Downloading link 13 of 20\n",
      "100% [..........................................................] 12908 / 12908snoozing for 4 before next link\n",
      "Downloading link 14 of 20\n",
      "100% [..........................................................] 12843 / 12843snoozing for 5 before next link\n",
      "Downloading link 15 of 20\n",
      "100% [..........................................................] 12881 / 12881snoozing for 3 before next link\n",
      "Downloading link 16 of 20\n",
      "100% [..........................................................] 12906 / 12906snoozing for 4 before next link\n",
      "Downloading link 17 of 20\n",
      "100% [..........................................................] 12816 / 12816snoozing for 6 before next link\n",
      "Downloading link 18 of 20\n",
      "100% [..........................................................] 12921 / 12921snoozing for 5 before next link\n",
      "Downloading link 19 of 20\n",
      "100% [..........................................................] 12901 / 12901snoozing for 5 before next link\n",
      "Downloading link 20 of 20\n",
      "100% [..........................................................] 13049 / 13049snoozing for 4 before next link\n"
     ]
    }
   ],
   "source": [
    "## downloading all documents\n",
    "links_total = len(doc_links)\n",
    "link_count = 1\n",
    "\n",
    "for link in doc_links:\n",
    "    print(f\"Downloading link {link_count} of {links_total}\")\n",
    "    link_count += 1\n",
    "    wget.download(link)\n",
    "    snoozer = randrange(3,7)\n",
    "    print(f\"snoozing for {snoozer} before next link\")\n",
    "    time.sleep(snoozer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de875e",
   "metadata": {},
   "source": [
    "# 2. Universal conversion function\n",
    "Rewrite your function from last week so it can do both:\n",
    "\n",
    "- take individual string values like ```$12.24267```, ```10,201``` and ```$12,501``` and convert them into floating point numbers like 12.24, 10201.0 and 12501.0\n",
    "\n",
    "- take string values in lists and convert them to floating point numbers. (reminder: you use a zip function).\n",
    "\n",
    "Test it on the numbers above and in this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6abb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list of string numbers\n",
    "string_numbers = [\"$12.24267\", \"10,201\", \"$12,501\", \"42,901\", \"$902,091\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c46ef8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StringConvert(a_string):\n",
    "    return round(float(a_string.replace(\"$\",\"\").replace(\",\",\"_\")),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4eef92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.24, 10201.0, 12501.0, 42901.0, 902091.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(StringConvert, string_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c931eec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.24"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StringConvert(\"$12.24267\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
